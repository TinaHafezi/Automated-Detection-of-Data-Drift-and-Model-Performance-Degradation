
# ğŸ“Š Automated Detection of Data Drift and Model Performance Degradation

This repository contains a complete **Machine Learning Monitoring System** capable of:

âœ”ï¸ Detecting distribution changes in input data (**Advanced Data Drift**)
âœ”ï¸ Detecting degradation in model performance after deployment
âœ”ï¸ Logging metrics into a database
âœ”ï¸ Visualizing results in an interactive dashboard
âœ”ï¸ Sending alerts (email/notifications) when issues occur
âœ”ï¸ Supporting multiple datasets (classification & regression)

This project was developed as part of a Bachelor's final project, and follows **industry-standard MLOps practices**.

---

## ğŸš€ Project Overview

Machine learning models often lose accuracy over time after deployment due to changing input data distributions or real-world conditions. This repository implements a modular monitoring pipeline that:

ğŸ”¹ Loads reference and current production data
ğŸ”¹ Detects **multi-layer data drift** using statistical tests and model-behavior monitoring
ğŸ”¹ Evaluates model performance over time
ğŸ”¹ Stores metric history
ğŸ”¹ Provides alerts when thresholds are exceeded
ğŸ”¹ Shows results via an interactive dashboard

---

## ğŸ§± Repository Structure

```text
.
â”œâ”€â”€ Train model/                       # Artifacts for Telco model
â”œâ”€â”€ Eth/                              # Artifacts for Ethereum model
â”‚
â”œâ”€â”€ drift_detection/                  # ğŸ”¥ Advanced Drift Monitoring Engine
â”‚   â”œâ”€â”€ drift_engine.py               # Main orchestrator for drift analysis
â”‚   â”œâ”€â”€ statistical_drift.py          # KS Test, Wasserstein Distance, PSI
â”‚   â”œâ”€â”€ data_quality.py               # Missing values, zero inflation, category drift
â”‚   â”œâ”€â”€ embedding_drift.py            # Model output / probability drift
â”‚   â””â”€â”€ prediction_drift.py           # Prediction distribution & confidence shift
â”‚
â”œâ”€â”€ data_loader.py
â”œâ”€â”€ dataset_adapters.py
â”œâ”€â”€ main.py
â”œâ”€â”€ model_monitor.py
â”œâ”€â”€ metrics_store.py
â”œâ”€â”€ risk_engine.py                    # System risk score computation
â”œâ”€â”€ alert_system.py
â”œâ”€â”€ config_loader.py
â”œâ”€â”€ dashboard.py
â”œâ”€â”€ fake_drift.py                     # Synthetic drift generator
â”œâ”€â”€ config.yaml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“Œ Features

### ğŸ“ˆ Advanced Automated Drift Detection

The system uses a **multi-layer drift detection framework** instead of a single metric:

| Drift Layer        | Method                           | Purpose                                  |
| ------------------ | -------------------------------- | ---------------------------------------- |
| Statistical Drift  | KS Test                          | Detects distribution shape change        |
| Statistical Drift  | Wasserstein Distance             | Measures magnitude of distribution shift |
| Statistical Drift  | PSI (Population Stability Index) | Monitors population stability            |
| Data Quality Drift | Missing/Zero/Category checks     | Detects data integrity issues            |
| Prediction Drift   | Output distribution monitoring   | Detects model behavior change            |
| Embedding Drift    | Probability space monitoring     | Detects concept drift signals            |

This provides **research-grade drift monitoring**, similar to production ML systems.

---

### ğŸ§  Dynamic Threshold Engineering

Instead of fixed thresholds only, the system supports:

```
Dynamic Threshold = Historical Mean + kÂ·Ïƒ
```

Used to detect:

* Sudden anomalies
* Gradual degradation
* Concept drift trends

---

### ğŸ’¡ Performance Monitoring

- Computes performance metrics over production data:
  - Classification: *Accuracy, F1, Precision, Recall*
  - Regression: *MAE, RMSE, RÂ²*
- Compares against baseline (validation) metrics
- Logs historic performance

---

### ğŸ› ï¸ Modular and Extensible


* Adapter-based inputs â€” add new datasets easily
* Separate data loader for each dataset
* Config-driven architecture (`config.yaml`)
* Designed for multiple tasks (classification & regression)
* **Pluggable drift detectors inside `drift_detection/`**

---
## ğŸ§ª How to Train Models

### â˜‘ï¸ Train Telco Model

```bash
python "Train model/train_model.py"
```

### â˜‘ï¸ Train Ethereum Model (1-hour)

```bash
python Eth/train_model.py
```

This will generate all required artifacts for monitoring.

---

## â–¶ï¸ Running the Monitoring Pipeline

Configure your dataset in `config.yaml`:

```yaml
app:
  dataset_name: "ethereum"         # "telco" or "ethereum"
model:
  path: "Eth/model.pkl"
  baseline_metrics_path: "Eth/baseline_metrics.csv"
monitoring:
  drift_threshold: 0.2
  performance_drop_threshold: 0.05
alerts:
  email_enabled: false
```

Then run:

```bash
python main.py
```

This script performs:

* Drift Detection
* Performance Evaluation
* Metrics Logging
* Optional Alerts

---
### ğŸ§ª Synthetic Drift Simulation

The system includes a **drift injection module** to simulate real-world failures:

| Dataset  | Simulated Drift                                         |
| -------- | ------------------------------------------------------- |
| Telco    | Feature distribution shift, label noise, missing values |
| Ethereum | Price shocks, volatility spikes, zero inflation         |

Run:

```bash
python fake_drift.py
```

---

## â–¶ï¸ Running the Monitoring Pipeline

This script now performs:

* **Statistical Drift Detection (KS, Wasserstein, PSI)**
* **Data Quality Drift Detection**
* **Prediction / Concept Drift Monitoring**
* Performance Evaluation
* Risk Score Calculation
* Metrics Logging
* Optional Alerts

---

## ğŸ“© Alerting & Notifications

The system supports email alerts via SMTP (e.g., Gmail). Use an **App Password** for security.

Configure in `config.yaml`:

```yaml
alerts:
  email_enabled: true
  sender_email: "example@gmail.com"
  sender_password: "app_password_here"
  receiver_email: "notify@domain.com"
```

Alerts trigger when:

âœ” Drift > threshold
âœ” Performance drop > threshold

---
## ğŸ“ Example Config (config.yaml)

```yaml
app:
  dataset_name: "telco"
model:
  path: "Train model/model.pkl"
  baseline_metrics_path: "Train model/baseline_metrics.csv"
monitoring:
  drift_threshold: 0.2
  performance_drop_threshold: 0.05
alerts:
  email_enabled: false
dashboard:
  host: "localhost"
  port: 8501
```

---
## ğŸ§  Why This Matters

This system demonstrates:

âœ” Multi-layer ML drift monitoring
âœ” Statistical + model-behavior monitoring
âœ” Dynamic threshold engineering
âœ” Data quality + prediction drift integration
âœ” End-to-end monitoring architecture

This mirrors **real-world production ML monitoring platforms** and modern research approaches.

---

Itâ€™s designed for **production readiness** and academic research use.

---

## ğŸ“¦ Requirements

Install dependencies:

```bash
pip install -r requirements.txt
```

---

## ğŸ“š Cite / References

If using this in academic work, reference:

âœ” Population Stability Index (PSI) for drift
âœ” Regression & Classification monitoring metrics

---

## ğŸ“¬ Contact

âœ¨ Created by **Tina Hafezi**
ğŸ“ Bachelor Final Project â€” ML Model Monitoring System
